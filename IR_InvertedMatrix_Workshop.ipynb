{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "890288b1",
   "metadata": {},
   "source": [
    "# 🛠️ Active Learning Workshop: Implementing an Inverted Matrix (Jupyter + GitHub Edition)\n",
    "## 🔍 Workshop Theme\n",
    "*Readable, correct, and collaboratively reviewed code—just like in the real world.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e78d18",
   "metadata": {},
   "source": [
    "\n",
    "Welcome to the 90-minute workshop! In this hands-on session, your team will build an **Inverted Index** pipeline, the foundation of many intelligent systems that need fast and relevant access to text data — such as AI agents.\n",
    "\n",
    "### 👥 Team Guidelines\n",
    "- Work in teams of 3.\n",
    "- Submit one completed Jupyter Notebook per team.\n",
    "- The final notebook must contain **Markdown explanations** and **Python code**.\n",
    "- Push your notebook to GitHub and share the `.git` link before class ends.\n",
    "\n",
    "---\n",
    "## 🔧 Workshop Tasks Overview\n",
    "\n",
    "1. **Document Collection**\n",
    "2. **Tokenizer Implementation**\n",
    "3. **Normalization Pipeline (Stemming, Stop Words, etc.)**\n",
    "4. **Build and Query the Inverted Index**\n",
    "\n",
    "> Each step includes a sample **talking point**. Your team must add your own custom **Markdown + code cells** with a **second talking point**, and test your Inverted Index with **2 phrase queries**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a922333",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 🧠 Learning Objectives\n",
    "- Implement an **Inverted Matrix** using real-world data during the NLP process.\n",
    "- Build **Jupyter Notebooks** with well-structured code and clear Markdown documentation.\n",
    "- Use **Git and GitHub** for collaborative version control and code sharing.\n",
    "- Identify and articulate coding issues (\"**talking points**\") and insert them directly into peer notebooks.\n",
    "- Practice **collaborative debugging**, professional peer feedback, and improve code quality.\n",
    "\n",
    "## 🧩 Workshop Structure (90 Minutes)\n",
    "1. **Instructor Use Case Introduction** *(15 min)* – Set up teams of 3 people. Read and understand the workshop, plus submission instructions. Seek assistance if needed.\n",
    "2. **Team Jupyter Notebook Development** *(45 min)* – Manual IR and Inverted Matrix coding + Markdown documentation (work as teams)\n",
    "3. **Push to GitHub** *(15 min)* – Teams commit and push initial notebooks. **Make sure to include your names so it is easy to identify the team that developed the Min-Max code**.\n",
    "4. **Instructor Review** - The instructor will go around, take notes, and provide coaching as needed, during the **Peer Review Round**\n",
    "5. **Email Delivery** *(15 min)* – Each team send the instructor an email **with the *.git link** to the GitHub repo **(one email/team)**. Subject on the email is: PROG8245 - Inverted Matrix  Workshop, Team #_____.\n",
    "\n",
    "\n",
    "## 💻 Submission Checklist\n",
    "- ✅ `IR_InvertedMatrix_Workshop.ipynb` with:\n",
    "  - Demo code: Document Collection, Tokenizer, Normalization Pipeline, and Inverted Index.\n",
    "  - Markdown explanations for each major step\n",
    "  - **Labeled talking point(s)** and 2 phrase query tests\n",
    "- ✅ `README.md` with:\n",
    "  - Dataset description\n",
    "  - Team member names\n",
    "  - Link to the dataset and license (if public)\n",
    "- ✅ GitHub Repo:\n",
    "  - Public repo named `IR-invertedmatrix-workshop`\n",
    "  - This is a group effort, so **choose one member of the team** to publish the repo\n",
    "  - At least **one commit containing one meaningful talking point**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e160c9d",
   "metadata": {},
   "source": [
    "## 📄 Step 1: Document Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc964464",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> We begin by gathering a text corpus. To build a robust index, your vocabulary should include **over 2000 unique words**. You can use scraped articles, academic papers, or open datasets.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Collect at least 20+ text documents.\n",
    "- Ensure the vocabulary exceeds 2000 unique words.\n",
    "- Load the documents into a list for processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "23ee0c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Documents loaded: 20\n",
      "✅ Unique vocabulary size: 6866\n",
      "🎯 Requirement satisfied: You can move to the next step!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# 📁 Path to the folder where the extracted files are stored\n",
    "folder_path = 'docs/'  # Replace with the path where you extract the ZIP\n",
    "\n",
    "# 🔄 Step 1: Load all .txt documents from folder\n",
    "def load_documents(folder_path):\n",
    "    documents = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# 🧹 Step 2: Clean text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Remove punctuation/special characters\n",
    "    return text.lower()\n",
    "\n",
    "# 📏 Step 3: Get vocabulary size\n",
    "def get_vocabulary_size(docs):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit(docs)\n",
    "    return len(vectorizer.vocabulary_)\n",
    "\n",
    "# 🚀 Run the pipeline\n",
    "documents = load_documents(\"./Data/\")\n",
    "cleaned_docs = [clean_text(doc) for doc in documents]\n",
    "vocab_size = get_vocabulary_size(cleaned_docs)\n",
    "\n",
    "# 📊 Output\n",
    "print(f\"✅ Documents loaded: {len(documents)}\")\n",
    "print(f\"✅ Unique vocabulary size: {vocab_size}\")\n",
    "\n",
    "# 💬 Optional check\n",
    "if len(documents) >= 20 and vocab_size >= 2000:\n",
    "    print(\"🎯 Requirement satisfied: You can move to the next step!\")\n",
    "else:\n",
    "    print(\"⚠️ Requirement NOT met — consider using longer/more diverse documents.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b342945a",
   "metadata": {},
   "source": [
    "## ✂️ Step 2: Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2803fb52",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> The tokenizer breaks raw text into a stream of words (tokens). This is the foundation for every later step in IR and NLP.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement a basic tokenizer that splits text into lowercase words.\n",
    "- Handle punctuation removal and basic non-alphanumeric filtering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "4806fc13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 tokens: ['usually', 'many', 'skin', 'finish', 'attorney', 'early', 'save', 'boy', 'in', 'store', 'thousand', 'pick', 'clear', 'today', 'face', 'far', 'system', 'star', 'stop', 'summer']\n",
      "Document 2 tokens: ['billion', 'trip', 'stand', 'stage', 'world', 'question', 'people', 'kid', 'price', 'determine', 'eight', 'join', 'whatever', 'friend', 'already', 'yet', 'fall', 'recent', 'it', 'account']\n",
      "Document 3 tokens: ['director', 'century', 'weight', 'statement', 'give', 'various', 'hot', 'similar', 'same', 'act', 'out', 'these', 'land', 'glass', 'three', 'world', 'either', 'mind', 'far', 'nice']\n",
      "Document 4 tokens: ['anyone', 'letter', 'particular', 'like', 'wind', 'whole', 'laugh', 'trip', 'room', 'keep', 'claim', 'ball', 'require', 'worker', 'standard', 'foreign', 'democratic', 'collection', 'skill', 'close']\n",
      "Document 5 tokens: ['best', 'there', 'prevent', 'option', 'among', 'candidate', 'raise', 'shake', 'without', 'customer', 'dog', 'religious', 'congress', 'per', 'dream', 'stuff', 'stock', 'perform', 'relate', 'team']\n",
      "Document 6 tokens: ['glass', 'enter', 'piece', 'third', 'put', 'deal', 'get', 'way', 'which', 'great', 'think', 'well', 'eight', 'lay', 'company', 'receive', 'catch', 'onto', 'special', 'voice']\n",
      "Document 7 tokens: ['crime', 'stage', 'grow', 'fight', 'best', 'pm', 'miss', 'good', 'sing', 'step', 'whose', 'ago', 'manage', 'this', 'available', 'standard', 'window', 'production', 'ball', 'understand']\n",
      "Document 8 tokens: ['north', 'song', 'perform', 'cause', 'region', 'individual', 'number', 'tend', 'stay', 'time', 'apply', 'indicate', 'party', 'mission', 'responsibility', 'wonder', 'six', 'among', 'individual', 'still']\n",
      "Document 9 tokens: ['window', 'way', 'on', 'result', 'make', 'according', 'indeed', 'step', 'family', 'executive', 'determine', 'medical', 'thing', 'skin', 'pass', 'nothing', 'reveal', 'should', 'general', 'glass']\n",
      "Document 10 tokens: ['whether', 'three', 'they', 'gun', 'writer', 'must', 'off', 'save', 'capital', 'account', 'care', 'heavy', 'after', 'southern', 'none', 'factor', 'dog', 'chair', 'hot', 'book']\n",
      "Document 11 tokens: ['election', 'sell', 'pretty', 'hot', 'story', 'include', 'left', 'also', 'subject', 'relationship', 'plant', 'goal', 'market', 'history', 'business', 'current', 'account', 'purpose', 'keep', 'trip']\n",
      "Document 12 tokens: ['skill', 'television', 'from', 'attorney', 'physical', 'financial', 'money', 'entire', 'prove', 'coach', 'sport', 'gun', 'federal', 'so', 'edge', 'really', 'article', 'box', 'drug', 'action']\n",
      "Document 13 tokens: ['turn', 'thing', 'player', 'last', 'garden', 'management', 'also', 'mention', 'growth', 'american', 'dog', 'single', 'fund', 'win', 'feeling', 'simply', 'wish', 'entire', 'argue', 'sign']\n",
      "Document 14 tokens: ['walk', 'attention', 'lose', 'even', 'let', 'religious', 'radio', 'oil', 'live', 'fine', 'various', 'message', 'list', 'blood', 'catch', 'one', 'through', 'friend', 'usually', 'whom']\n",
      "Document 15 tokens: ['door', 'until', 'whether', 'occur', 'avoid', 'policy', 'although', 'present', 'suddenly', 'training', 'growth', 'important', 'might', 'wonder', 'animal', 'back', 'well', 'character', 'happy', 'food']\n",
      "Document 16 tokens: ['relationship', 'girl', 'human', 'often', 'particular', 'beat', 'left', 'than', 'condition', 'accept', 'along', 'common', 'what', 'cause', 'generation', 'mouth', 'deep', 'age', 'wind', 'develop']\n",
      "Document 17 tokens: ['read', 'college', 'event', 'next', 'our', 'up', 'spring', 'management', 'source', 'senior', 'person', 'drug', 'move', 'drug', 'speech', 'instead', 'effort', 'inside', 'exist', 'control']\n",
      "Document 18 tokens: ['whether', 'instead', 'red', 'over', 'full', 'raise', 'unit', 'camera', 'challenge', 'season', 'officer', 'lawyer', 'fill', 'role', 'ten', 'these', 'break', 'can', 'of', 'fight']\n",
      "Document 19 tokens: ['save', 'statement', 'really', 'responsibility', 'growth', 'month', 'agency', 'race', 'attack', 'sound', 'clear', 'walk', 'quality', 'range', 'soon', 'yard', 'account', 'employee', 'tax', 'statement']\n",
      "Document 20 tokens: ['raise', 'loss', 'employee', 'occur', 'become', 'professional', 'buy', 'house', 'moment', 'should', 'not', 'art', 'store', 'base', 'executive', 'color', 'provide', 'individual', 'one', 'you']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Load documents from 'docs/' folder\n",
    "def load_documents(folder_path='docs/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Example usage: load all docs and tokenize each\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    tokens = basic_tokenizer(doc)\n",
    "    print(f\"Document {i} tokens: {tokens[:20]}\")  # print first 20 tokens of each doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03ed76ec",
   "metadata": {},
   "source": [
    "## 🔁 Step 3: Normalization Pipeline (Stemming, Stop Word Removal, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f277a0d",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> Now we normalize tokens: convert to lowercase, remove stop words, apply stemming or affix stripping. This reduces redundancy and enhances search accuracy.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Use `nltk` to remove stopwords and apply stemming.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "66ae9a94",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1 normalized tokens: ['usual', 'mani', 'skin', 'finish', 'attorney', 'earli', 'save', 'boy', 'store', 'thousand', 'pick', 'clear', 'today', 'face', 'far', 'system', 'star', 'stop', 'summer', 'film']\n",
      "Document 2 normalized tokens: ['billion', 'trip', 'stand', 'stage', 'world', 'question', 'peopl', 'kid', 'price', 'determin', 'eight', 'join', 'whatev', 'friend', 'alreadi', 'yet', 'fall', 'recent', 'account', 'mother']\n",
      "Document 3 normalized tokens: ['director', 'centuri', 'weight', 'statement', 'give', 'variou', 'hot', 'similar', 'act', 'land', 'glass', 'three', 'world', 'either', 'mind', 'far', 'nice', 'manag', 'continu', 'surfac']\n",
      "Document 4 normalized tokens: ['anyon', 'letter', 'particular', 'like', 'wind', 'whole', 'laugh', 'trip', 'room', 'keep', 'claim', 'ball', 'requir', 'worker', 'standard', 'foreign', 'democrat', 'collect', 'skill', 'close']\n",
      "Document 5 normalized tokens: ['best', 'prevent', 'option', 'among', 'candid', 'rais', 'shake', 'without', 'custom', 'dog', 'religi', 'congress', 'per', 'dream', 'stuff', 'stock', 'perform', 'relat', 'team', 'actual']\n",
      "Document 6 normalized tokens: ['glass', 'enter', 'piec', 'third', 'put', 'deal', 'get', 'way', 'great', 'think', 'well', 'eight', 'lay', 'compani', 'receiv', 'catch', 'onto', 'special', 'voic', 'win']\n",
      "Document 7 normalized tokens: ['crime', 'stage', 'grow', 'fight', 'best', 'pm', 'miss', 'good', 'sing', 'step', 'whose', 'ago', 'manag', 'avail', 'standard', 'window', 'product', 'ball', 'understand', 'view']\n",
      "Document 8 normalized tokens: ['north', 'song', 'perform', 'caus', 'region', 'individu', 'number', 'tend', 'stay', 'time', 'appli', 'indic', 'parti', 'mission', 'respons', 'wonder', 'six', 'among', 'individu', 'still']\n",
      "Document 9 normalized tokens: ['window', 'way', 'result', 'make', 'accord', 'inde', 'step', 'famili', 'execut', 'determin', 'medic', 'thing', 'skin', 'pass', 'noth', 'reveal', 'gener', 'glass', 'loss', 'risk']\n",
      "Document 10 normalized tokens: ['whether', 'three', 'gun', 'writer', 'must', 'save', 'capit', 'account', 'care', 'heavi', 'southern', 'none', 'factor', 'dog', 'chair', 'hot', 'book', 'strong', 'factor', 'budget']\n",
      "Document 11 normalized tokens: ['elect', 'sell', 'pretti', 'hot', 'stori', 'includ', 'left', 'also', 'subject', 'relationship', 'plant', 'goal', 'market', 'histori', 'busi', 'current', 'account', 'purpos', 'keep', 'trip']\n",
      "Document 12 normalized tokens: ['skill', 'televis', 'attorney', 'physic', 'financi', 'money', 'entir', 'prove', 'coach', 'sport', 'gun', 'feder', 'edg', 'realli', 'articl', 'box', 'drug', 'action', 'go', 'usual']\n",
      "Document 13 normalized tokens: ['turn', 'thing', 'player', 'last', 'garden', 'manag', 'also', 'mention', 'growth', 'american', 'dog', 'singl', 'fund', 'win', 'feel', 'simpli', 'wish', 'entir', 'argu', 'sign']\n",
      "Document 14 normalized tokens: ['walk', 'attent', 'lose', 'even', 'let', 'religi', 'radio', 'oil', 'live', 'fine', 'variou', 'messag', 'list', 'blood', 'catch', 'one', 'friend', 'usual', 'media', 'guess']\n",
      "Document 15 normalized tokens: ['door', 'whether', 'occur', 'avoid', 'polici', 'although', 'present', 'suddenli', 'train', 'growth', 'import', 'might', 'wonder', 'anim', 'back', 'well', 'charact', 'happi', 'food', 'place']\n",
      "Document 16 normalized tokens: ['relationship', 'girl', 'human', 'often', 'particular', 'beat', 'left', 'condit', 'accept', 'along', 'common', 'caus', 'gener', 'mouth', 'deep', 'age', 'wind', 'develop', 'director', 'friend']\n",
      "Document 17 normalized tokens: ['read', 'colleg', 'event', 'next', 'spring', 'manag', 'sourc', 'senior', 'person', 'drug', 'move', 'drug', 'speech', 'instead', 'effort', 'insid', 'exist', 'control', 'degre', 'well']\n",
      "Document 18 normalized tokens: ['whether', 'instead', 'red', 'full', 'rais', 'unit', 'camera', 'challeng', 'season', 'offic', 'lawyer', 'fill', 'role', 'ten', 'break', 'fight', 'administr', 'ask', 'coupl', 'democrat']\n",
      "Document 19 normalized tokens: ['save', 'statement', 'realli', 'respons', 'growth', 'month', 'agenc', 'race', 'attack', 'sound', 'clear', 'walk', 'qualiti', 'rang', 'soon', 'yard', 'account', 'employe', 'tax', 'statement']\n",
      "Document 20 normalized tokens: ['rais', 'loss', 'employe', 'occur', 'becom', 'profession', 'buy', 'hous', 'moment', 'art', 'store', 'base', 'execut', 'color', 'provid', 'individu', 'one', 'movement', 'method', 'behavior']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Download stopwords once\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "    return stemmed_tokens\n",
    "\n",
    "def load_documents(folder_path='./Data/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents\n",
    "\n",
    "# Load and normalize all documents\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "for i, doc in enumerate(docs, 1):\n",
    "    normalized_tokens = normalize_tokens(doc)\n",
    "    print(f\"Document {i} normalized tokens: {normalized_tokens[:20]}\")  # first 20 tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a34cf58",
   "metadata": {},
   "source": [
    "## 🔍 Step 4: Inverted Index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "847c39dd",
   "metadata": {},
   "source": [
    "\n",
    "    ### 🗣 Instructor Talking Point:\n",
    "    > We now map each normalized token to the list of document IDs in which it appears. This is the core structure that allows fast Boolean and phrase queries.\n",
    "\n",
    "    ### 🔧 Your Task:\n",
    "    - Build the inverted index using a dictionary.\n",
    "    - Add code to support phrase queries using positional indexing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8ca8f106",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\acer\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample tokens from inverted index:\n",
      "usual: [0, 2, 6, 8, 11, 12, 13, 16, 18]\n",
      "mani: [0, 1, 2, 3, 9, 12, 13, 16, 18]\n",
      "skin: [0, 1, 3, 4, 6, 7, 8, 10, 12, 15, 16]\n",
      "finish: [0, 3, 5, 11, 13, 17, 19]\n",
      "attorney: [0, 3, 4, 7, 8, 9, 10, 11, 13, 14, 15, 16]\n",
      "earli: [0, 2, 5, 7, 17]\n",
      "save: [0, 2, 3, 4, 5, 7, 8, 9, 12, 13, 18]\n",
      "boy: [0, 1, 2, 3, 4, 8, 9, 10, 14, 15, 19]\n",
      "store: [0, 5, 6, 7, 8, 11, 14, 16, 17, 19]\n",
      "thousand: [0, 2, 3, 5, 8, 10, 12, 13, 19]\n",
      "\n",
      "Documents containing the phrase 'machine learning': [4, 11]\n",
      "\n",
      "Documents containing the phrase 'artificial intelligence': [3, 11, 17]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import nltk\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "def normalize_tokens(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    tokens = text.split()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    filtered_stemmed = [stemmer.stem(t) for t in tokens if t not in stop_words]\n",
    "    return filtered_stemmed\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = {}\n",
    "    for doc_id, text in enumerate(docs):\n",
    "        tokens = normalize_tokens(text)\n",
    "        seen_in_doc = set()\n",
    "        for pos, token in enumerate(tokens):\n",
    "            # Add positional info for phrase queries\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token][doc_id] = []\n",
    "            inverted_index[token][doc_id].append(pos)\n",
    "            seen_in_doc.add(token)\n",
    "    return inverted_index\n",
    "\n",
    "def phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "    positions_lists = []\n",
    "    for token in phrase_tokens:\n",
    "        if token not in inverted_index or doc_id not in inverted_index[token]:\n",
    "            return False\n",
    "        positions_lists.append(inverted_index[token][doc_id])\n",
    "\n",
    "    # Check positions for sequential occurrence of phrase tokens\n",
    "    first_positions = positions_lists[0]\n",
    "    for start_pos in first_positions:\n",
    "        if all((start_pos + offset) in positions_lists[offset] for offset in range(1, len(positions_lists))):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def phrase_search(inverted_index, phrase, docs):\n",
    "    phrase_tokens = normalize_tokens(phrase)\n",
    "    matched_docs = []\n",
    "    for doc_id in range(len(docs)):\n",
    "        if phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "            matched_docs.append(doc_id)\n",
    "    return matched_docs\n",
    "\n",
    "def load_documents(folder_path='./Data/'):\n",
    "    documents = []\n",
    "    for filename in sorted(os.listdir(folder_path)):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as f:\n",
    "                documents.append(f.read())\n",
    "    return documents\n",
    "\n",
    "# ---- Main ----\n",
    "docs = load_documents('./Data/')\n",
    "\n",
    "inverted_index = build_inverted_index(docs)\n",
    "\n",
    "print(\"Sample tokens from inverted index:\")\n",
    "for token, postings in list(inverted_index.items())[:10]:\n",
    "    print(f\"{token}: {list(postings.keys())}\")\n",
    "\n",
    "# Test phrase queries\n",
    "phrases = [\"machine learning\", \"artificial intelligence\"]\n",
    "\n",
    "for phrase in phrases:\n",
    "    matched = phrase_search(inverted_index, phrase, docs)\n",
    "    print(f\"\\nDocuments containing the phrase '{phrase}': {matched}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faef4df8",
   "metadata": {},
   "source": [
    "## 🧪 Test: Phrase Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db832216",
   "metadata": {},
   "source": [
    "\n",
    "### 🗣 Instructor Talking Point:\n",
    "> A phrase query requires the exact sequence of terms (e.g., \"machine learning\"). To support this, extend the inverted index to store positions, not just docIDs.\n",
    "\n",
    "### 🔧 Your Task:\n",
    "- Implement 2 phrase queries.\n",
    "- Demonstrate that they return the correct documents.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "97132fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents containing phrase 'machine learning': [0, 1, 2]\n",
      "Documents containing phrase 'deep learning': [1]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def basic_tokenizer(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "def build_inverted_index(docs):\n",
    "    inverted_index = {}\n",
    "    for doc_id, text in enumerate(docs):\n",
    "        tokens = basic_tokenizer(text)\n",
    "        for pos, token in enumerate(tokens):\n",
    "            if token not in inverted_index:\n",
    "                inverted_index[token] = {}\n",
    "            if doc_id not in inverted_index[token]:\n",
    "                inverted_index[token][doc_id] = []\n",
    "            inverted_index[token][doc_id].append(pos)\n",
    "    return inverted_index\n",
    "\n",
    "def phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "    positions_lists = []\n",
    "    for token in phrase_tokens:\n",
    "        if token not in inverted_index or doc_id not in inverted_index[token]:\n",
    "            return False\n",
    "        positions_lists.append(inverted_index[token][doc_id])\n",
    "    first_positions = positions_lists[0]\n",
    "    for start_pos in first_positions:\n",
    "        if all((start_pos + offset) in positions_lists[offset] for offset in range(1, len(positions_lists))):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def phrase_search(inverted_index, phrase, docs):\n",
    "    phrase_tokens = basic_tokenizer(phrase)\n",
    "    matched_docs = []\n",
    "    for doc_id in range(len(docs)):\n",
    "        if phrase_in_doc(inverted_index, phrase_tokens, doc_id):\n",
    "            matched_docs.append(doc_id)\n",
    "    return matched_docs\n",
    "\n",
    "# Sample documents\n",
    "docs = [\n",
    "    \"Machine learning is fascinating.\",\n",
    "    \"Deep learning is a subset of machine learning.\",\n",
    "    \"Artificial intelligence includes machine learning.\",\n",
    "    \"Learning about machine algorithms.\"\n",
    "]\n",
    "\n",
    "# Build index\n",
    "index = build_inverted_index(docs)\n",
    "\n",
    "# Phrase queries\n",
    "phrases = [\"machine learning\", \"deep learning\"]\n",
    "\n",
    "for phrase in phrases:\n",
    "    matched = phrase_search(index, phrase, docs)\n",
    "    print(f\"Documents containing phrase '{phrase}': {matched}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
